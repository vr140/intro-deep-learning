{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Introduction to Non Deep Learning Methods\n",
    "\n",
    "### 1. Linear Classifier\n",
    "\n",
    "#### 1.1 Classification vs Regression\n",
    "\n",
    "Traditionally, machine learning can be thought of as statistical dataset learning. That is, a system \"learns\" by building a statistical model from its \"experience,\" which is provided to it in the form of a dataset. Machine learning has been used for a multitude of tasks in areas such as diagnosis, recognition, and planning. We can simplify by categorizing all of these tasks as providing some kind of prediction: in the case of diagnosis, we are predicting something diagnostic, in the case of recognition, we are predicting in the form of recognizing an object or pattern, and in the case of planning, we are predicting the next optimal move in our plan. Broadly, this prediction can come in the form of classification and regression. The below image visually depicts the difference [8]:\n",
    "\n",
    "<img src=\"classification_vs_regression.jpg\" width=\"600\" height=\"480\" />\n",
    "\n",
    "\n",
    "While classification predicts a binary label (between 0 and 1 or 1 and -1 depending on how the problem frames it), regression predicts a numerical value (any floating point number). The image above in particular shows a line or linear classifier on the left, which separates the positive (typically labeled 1) and negative labels (typically labelled either 0 or -1). The linear regressor on the right models a line that fits a set of numerical values.\n",
    "\n",
    "The extension of classification above to multiple classes is simply extending the label set from {0, 1} to {0, 1, ... K} where K = the number of possible classes. This is known as multiclass classification.\n",
    "\n",
    "We will focus on multiclass classification in this notebook, but note that we could easily simplify this to the case of multiple classes with only two classes to arrive at binary classification.\n",
    "\n",
    "#### 1.2 Motivating example\n",
    "\n",
    "**NOTE: The rest of the notebook will be primarily derived from CS231N.**\n",
    "\n",
    "A classic problem that requires multiclass classification is image classification, where you have to come up with a way to take an input image and come up with a single label (or a distribution as shown below)  that captures what is in the image, drawn from a set of predefined image labels [2]. \n",
    "\n",
    "<img src=\"classify.jpg\" />\n",
    "\n",
    "\n",
    "In the image above, \"an image classification _model_ takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers.\"  For the purposes of this notebook, we will assume the image was drawn from the CIFAR-10 dataset [9] and was enlarged for clarity. The CIFAR-10 \"dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example 'airplane', 'automobile', 'bird', etc). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images.\" [2]\n",
    "\n",
    "Let us assume the original cat image (the tiny one that was not enlarged) is 32 pixels wide, 32 pixels tall, and has three color channels Red, Green, Blue (or RGB). Therefore, the image consists of 32 x 32 x 3 numbers for a total of 3072 numbers. Each number represents a color channel of a pixel and its value ranges from 0 (black) to 255 (white). Thus, if we wanted to perform image classification, our task would be to turn the image, represented as 3072 numbers, into a single label, such as 'cat.' \n",
    "\n",
    "We will use the _training set_ of example images later on to _train_ a classification model and then evaluate its performance on a set of images the model has not encountered, the _test set_. Why does this work theoretically speaking? It is because we assume the examples are drawn from an independent and identically distributed set of images (this is known as the i.i.d assumption). Thus, the model should in theory be able to _generalize_ to a test dataset given a training dataset [4]. (Note: there is also a third kind of dataset, known as a validation dataset, that is typically seen in machine learning literature. The validation dataset is a subset of the training dataset that we use to help us pick various _hyperparameters_ of whatever model we pick. More on this later.)\n",
    "\n",
    "#### 1.3 Linear Regression function\n",
    "\n",
    "**Warning: Math below! **\n",
    "\n",
    "Our initial approach to image classification will naturally extend to deep learning. \"The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.\" [2]\n",
    "\n",
    "Let us assume we have a matrix of input features $\\boldsymbol{X}$, consisting of m (m = number of examples) rows, where each row $\\boldsymbol{x}^{(i)}$ corresponds to a vector of n feature values $[\\boldsymbol{x}^{(i)}_{1}, \\boldsymbol{x}^{(i)}_{2}, ... \\boldsymbol{x}^{(i)}_{n}]$.  Thus, $\\boldsymbol{X}$ is a m x n matrix. We also have a vector of m labels $\\boldsymbol{y}$, with $y_{i}$ corresponding to the _class_ or _label_ for example $i$. The score function will take the form of a linear regression function:\n",
    "\n",
    "$\\hat{y} = \\boldsymbol{w}^{T}\\boldsymbol{x} + b$\n",
    "\n",
    "\n",
    "$\\hat{y}$ is the value our classification model predicts $y$ should take on [4].\n",
    "\n",
    "$\\boldsymbol{x}$ corresponds to a row of input features from our matrix $\\boldsymbol{X}$ above and $\\boldsymbol{w}$ is a vector of n _weights_ that determine how each feature in $\\boldsymbol{x}$ affects the prediction. A positive weight means increasing the value of that feature increase our predicted value $\\hat{y}$, and a negative weight means increasing the value of that feature decreases our predicted value. And if a feature has a weight of zero, it means it does not have any impact on the prediction.\n",
    "\n",
    "$b$ refers to a bias parameter, which is the value of the prediction if all of the weights were 0.\n",
    "\n",
    "If we wanted to extend this to multi-class, we would write w as W since it would now be a matrix of weights, one row for each class:\n",
    "\n",
    "$\\hat{y} = \\boldsymbol{W}^{T}\\boldsymbol{x} + b$\n",
    "\n",
    "We can simplify the equation to:\n",
    "\n",
    "$\\hat{y} = \\boldsymbol{W}^{T}\\boldsymbol{x}$\n",
    "\n",
    "if we assume that $\\boldsymbol{x}$ is augmented by an extra entry that is always set to 1 and the weight corresponding to this extra entry is the bias parameter.  This is known as the bias trick and is illustrated below [2]:\n",
    "\n",
    "<img src=\"wb.jpg\" width=\"600\" height=\"480\" / >\n",
    "\n",
    "Our task is thus to learn a set of weights $\\boldsymbol{w}$ that results in the best model performance. How do we measure model performance? There was a number of different measures, and one common one is to the compute the mean squared error (MSE) of the model on the test test.  The $MSE_{test}$ is given by [4]:\n",
    "\n",
    "$MSE_{test}$ = $\\frac{1}{m} \\sum_{i=1} ( \\hat{y}^{(test)} - {y}^{(test)} )_{i}^{2}$\n",
    "\n",
    "where $\\hat{y}^{(test)}$ is the predictions of the model on the test set and ${y}^{(test)}$ is the vector of actual regression _targets_ from the test set.  Thus, mean squared error is essentially a sum of a function of prediction - target over all the input exmaples.\n",
    "\n",
    "Remember that while we evaluate the model performance on the test set, the model only gets to see the training set. Thus, we can only work to minimize $MSE_{train}$ and then see if it generalizes to the test set. The nice thing about linear regression is that you can actually solve a system of equations (that will not be described here) for a set of weights $\\boldsymbol{w}$ that minimizes $MSE_{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiclass Support Vector Machine Classifier\n",
    "\n",
    "Note that in the section above we initially started talking about classfication, which requires a model to be able to predict 0/1, and then we ended up introduction linear regression, which actually outputs a score (that is not necessarily between 0 and 1).  How do we reconcile this?\n",
    "\n",
    "We can use the image classification use case to understand how to combine these two ideas together.\n",
    "\n",
    "<img src=\"imagemap.jpg\" />\n",
    "\n",
    "The above is \"an example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it's looking at a dog.\" [2]\n",
    "\n",
    "**The class (or label) with the highest score is the one the model predicts for the image** We are going to measure how well the model does with a _loss function_, also known as a cost function or objective function.  The loss should be low if we are doing a good job of classifying the training data and high if we are not doing a good job.\n",
    "\n",
    "Below are notes that detail a common loss function [2]:\n",
    "\n",
    "<img src=\"multiclass_svm_loss_1.jpg\" />\n",
    "\n",
    "<img src=\"multiclass_svm_loss_2.jpg\" />\n",
    "\n",
    "\n",
    "In the next cell, you will implement the loss function in Python using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  # TODO: compute and return loss. Assume delta is fixed at 1.0 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multinomial Logistic Regression aka Softmax Classifier\n",
    "\n",
    "Support Vector Machine Classifier :: Multiclass Support Vector Machine Classifier\n",
    "\n",
    "as\n",
    "\n",
    "Logistic Regression Classifier :: Multinomial Logistic Regression Classifier (also known as Softmax classifier)\n",
    "\n",
    "That's the analogy to keep in mind. The Support Vector Machine Classifier and Logistic Regression Classifier are for the binary classification use case: the former outputs a class label whereas the latter outputs a probability of the positive class label.  The multiclass Support Vector Machine Classifier and multinomial Logistic Regression Classifier are for the multiclass classification use case: the former outputs a set of class scores whereas the latter outputs a probability distribution across all the labels. In the multiclass class, you can get the predicted label by finding the class with the highest score or highest probability respectively.\n",
    "\n",
    "Below are notes on the loss function used to construct the softmax classifier [2]:\n",
    "\n",
    "<img src=\"cross_entropy_loss.jpg\" />\n",
    "\n",
    "\n",
    "The Softmax classifier is minimizing the \"cross-entropy\" loss (fancy term for difference between two probability distributions) between the estimated class probabilities and the actual distribution, which we take to be a distribution where probability = 1 on the correct class and 0 elsewhere (i.e. p=[0,…1,…,0] contains a single 1 at the yi -th position corresponding to the correct label). \n",
    "\n",
    "In the next cell, you will implement the loss function in Python using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  Compute the cross-entropy loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  # TODO: compute and return loss. Hint: Use numpy.exp(x) to compute e^x.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What goes into a machine learning algorithm?\n",
    "\n",
    "As Goodfellow et. al states, **nearly all machine learning algorithms can be characterized by the following recipe: \"a specification of a dataset, a cost function, an optimization procedure and a model.\"**[6] We can mix and match these to produce different algorithms. In the case of deep learning, our model will be a neural network, our optimization procedure will be typically something called stochastic gradient descent, and our cost/loss function can come from anything in the family of classification loss functions. We already saw two classification loss functions above: hinge loss and cross-entropy loss. There will be more to come on components of this recipe in the next few notebooks!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. References\n",
    "<pre>\n",
    "  [1] Fast.ai (http://course.fast.ai/)  \n",
    "  [2] CS231N (http://cs231n.github.io/)  \n",
    "  [3] CS224D (http://cs224d.stanford.edu/syllabus.html)  \n",
    "  [4] Hands on Machine Learning (https://github.com/ageron/handson-ml)  \n",
    "  [5] Deep learning with Python Notebooks (https://github.com/fcholletdeep-learning-with-python-notebooks)  \n",
    "  [6] Deep learning by Goodfellow et. al (http://www.deeplearningbook.org/)  \n",
    "  [7] Neural networks online book (http://neuralnetworksanddeeplearning.com/)  \n",
    "  [8] https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf\n",
    "  [9] http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
