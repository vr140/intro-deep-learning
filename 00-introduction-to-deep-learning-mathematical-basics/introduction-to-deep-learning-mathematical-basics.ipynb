{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Deep Learning Mathematical Basics\n",
    "\n",
    "### 1. Mathematical Concepts\n",
    "\n",
    "Below we will cover the basic mathematical concepts of deep learning. I've tried to cover as minimal as possible to get you feeling familiar with the notation and rules involved.\n",
    "\n",
    "\n",
    "**NOTE: You do not have to learn all of this math in order to be successful at deep learning. I encourage you to skip to the next notebook and come back here. The math is useful to understand gradient descent and regularization in this notebook and backpropagation in the next notebook. Also, if you take time to understand, it will get you feeling comfortable enough to read deep learning posts and possibly even academic papers for new ideas.**\n",
    "\n",
    "#### 1.1 Vectors and Matrices\n",
    "\n",
    "A _vector_ is technically defined as something that has a magnitude and direction (in contrast to a _scalar_, which is just a number and only has magnitude). For the purposes of this notebook, a vector will represent a vertical _array_ of items, with dimensions m x 1 (or m rows and 1 column):\n",
    "\n",
    "$\\boldsymbol{x} = \\begin{bmatrix} \n",
    "x_{1} \\\\ \n",
    "x_{2} \\\\ \n",
    "... \\\\\n",
    "x_{m} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Notice we said \"items.\" An item could be a number, a variable, a function, anything... For now, think of these as scalars (eg x = [1,2,3]). **Just remember that a vector is just a data structure and it can contain whatever we want inside of it to suit our particular needs. You can have a vector of scalars, vector of variables, vector of functions, and even vector of vectors!**\n",
    "\n",
    "Note that the vector notation we use is a bolded $\\boldsymbol{x}$ (you may also see $\\vec{v}$ or $\\hat{v}$ but we will avoid those forms).  In contrast, a non-bolded x will represent a scalar (eg x = 6). If we wanted a horizontal array, we would write it as:\n",
    "\n",
    "$\\boldsymbol{x}^{T} = \\begin{bmatrix} \n",
    "x_{1} x_{2} ... x_{m}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "A capitalized bolded $\\boldsymbol{X}$ represents a _matrix_, which in our case will be used to represent a two dimensional m x n object, with m rows and n columns:\n",
    "\n",
    "$\\boldsymbol{X} = \\begin{bmatrix} \n",
    "x_{1,1} & x_{1,2} & ... & x_{1,n} \\\\ \n",
    "x_{2,1} & x_{2,2} & ... & x_{2,n} \\\\ \n",
    "... \\\\\n",
    "x_{m,1} & x_{m,2} & ... & x_{m,n} \n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Derivatives\n",
    "\n",
    "\n",
    "Before we get to derivatives of vectors and matrices, let's talk about single variable derivative rules. if you feel like you need a review basic single variable derivative rules check out the videos in:\n",
    "https://www.khanacademy.org/math/old-ap-calculus-ab/ab-derivative-rules\n",
    "\n",
    "You can also see https://www.khanacademy.org/math/differential-calculus/dc-diff-intro for more rules. \n",
    "\n",
    "Below are the basic  derivative rules if you just need a reference:\n",
    "<img src=\"scalar_derivative_rules.jpg\" width=\"600\" height=\"480\" />\n",
    "\n",
    "\n",
    "The below explains how to think of the derivative d/dx. It seems scary but it's no different than + or * because it is just another operator:\n",
    "<img src=\"d_dx.jpg\" width=\"600\" height=\"480\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Gradient\n",
    "\n",
    "In the above rules, the function was a paramater of a single variable x. How we compute derivatives if there are multiple parameters (say x,y)? Well, it turns out we can only compute a _partial derivative_ with respect to one variable at a time. The notation used for partial derivative will be the following:\n",
    "<p>\n",
    "$\\frac{\\partial f(x,y)}{\\partial x}$ - partial derivative of f(x,y) with respect to x\n",
    "<p>\n",
    "$\\frac{\\partial f(x,y)}{\\partial y}$ - partial derivative of f(x,y) with respect to y\n",
    "<p>\n",
    "\n",
    "\n",
    "Using the example in [9], let's say $f(x,y) = 3x^{2}y$\n",
    "\n",
    "To get $\\frac{\\partial f(x,y)}{\\partial x}$ , we have to treat y as a constant and use the single variable derivative rules:\n",
    "\n",
    "\n",
    "$\\frac{\\partial f(x,y)}{\\partial x} = 6yx$\n",
    "\n",
    "Similarly, to get $\\frac{\\partial f(x,y)}{\\partial y}$ , we have to treat x as a constant and use the single variable derivative rules:\n",
    "\n",
    "\n",
    "$\\frac{\\partial f(x,y)}{\\partial y} = 3x^{2}$\n",
    "\n",
    "When you see the word _gradient_, remember that the gradient is a vector. Specifically, it's a vector of the partial derivatives of a function (\"vector of partials\" for short).\n",
    "\n",
    "$\\Large\\nabla f(x,y) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x,y)}{\\partial y}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**What if we have multiple functions?**\n",
    "\n",
    "Let's say you also had another function $g(x,y) = 2x + y^{8}$ in addition to f(x,y) defined above.\n",
    "Then you stack the gradients in a matrix called the _Jacobian_ (we use the notation J for the Jacobian):\n",
    "\n",
    "<img src=\"simple_jacobian.jpg\" width=\"600\" height=\"480\" />\n",
    "\n",
    "That's a sample of matrix calculus!\n",
    "\n",
    "**What if we have a lot of parameters in each of our functions?**\n",
    "\n",
    "In the case of two parameters, we can manually write out the gradient for the function and consequently the Jacobian of multiple functions. However, we need a way to generalize this to a lot of parameters because neural networks often have a lot of parameters (eg weights). \n",
    "\n",
    "Let's say you had a function with many parameters $f(a,b,c...)$. We can rewrite it as $f(\\boldsymbol{x})$ (remember the bold symbol means a vector) where\n",
    "\n",
    "$\\boldsymbol{x} = \\begin{bmatrix} \n",
    "a \\\\ \n",
    "b \\\\ \n",
    "c \\\\ \n",
    "... \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "which can be rewritten as:\n",
    "\n",
    "$\\boldsymbol{x} = \\begin{bmatrix} \n",
    "x_{1} \\\\ \n",
    "x_{2} \\\\ \n",
    "x_{3} \\\\ \n",
    "... \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "NOTE: A initially confusing part may be that in the previous section, each of the elements in the vector (x1, x2, x3) represented a scalar (or number) whereas now each of these elements is actually a variable. \n",
    "\n",
    "Now, we can write our gradient of the vector valued function $f(\\boldsymbol{x})$ as:\n",
    "\n",
    "$\\Large\\nabla f(\\boldsymbol{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_{1}}, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_{2}}, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_{3}}, ...\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**What if we have multiple functions, each with lots of parameters?**\n",
    "\n",
    "If we have multiple functions, $f_{1}$, $f_{2}$, ..., then we can use the Jacobian to get all the gradients! The Jacobian is just a stack of gradients.\n",
    "\n",
    "We can define a vector of functions $\\boldsymbol{y}$ as:\n",
    "\n",
    "$\\boldsymbol{y} = \\begin{bmatrix} \n",
    "f_{1}(\\boldsymbol{x}) \\\\ \n",
    "f_{2}(\\boldsymbol{x}) \\\\ \n",
    "f_{3}(\\boldsymbol{x}) \\\\ \n",
    "... \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\Large\n",
    "J = \\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}\n",
    "$\n",
    "\n",
    "The Jacobian matrix is a stack of m Ã— n possible partial derivatives where m = number of functions in the vector $\\boldsymbol{y}$ and n = number of variables in the vector $\\boldsymbol{x}$.\n",
    "\n",
    "<img src=\"complex_jacobian.jpg\" width=\"600\" height=\"480\" />\n",
    "\n",
    "**Can we summarize all this?**\n",
    "\n",
    "<p>\n",
    "For a function with a single variable $f(x)$, we use the derivative rules.\n",
    "<p>\n",
    "For a function with two variables $f(x, y)$, we use the partial derivative rules to make a gradient (vector of all the partial derivatives)\n",
    "<p>\n",
    "For two functions with two variables $f(x,y)$ and $g(x,y)$, we use the Jacobian to stack the two gradients.\n",
    "<p>\n",
    "For a function with n variables $f(\\boldsymbol{x})$, we use the partial derivative rules to make a gradient (vector of all the partial derivatives).\n",
    "<p>\n",
    "For m functions with n variables $\\boldsymbol{y}(\\boldsymbol{x})$, we use the Jacobian to stack the m gradients (each of which has npartials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Chain Rule\n",
    "\n",
    "Good news! We don't have to learn another rule. We just have to know that such a thing as the \"chain rule\" exists (you can look it up if you'd like using [9] and [10] or Khan academy videos above).  \n",
    "\n",
    "The chain rule is what allows us to get the derivative of a function wrapping a single variable function:\n",
    "<p>\n",
    "$ y = f(g(x))$<p>\n",
    "$ u = g(x)$<p>\n",
    "$ y = f(u)$<p>\n",
    "\n",
    "and the derivatives of a function wrapping a vector of single variable functions $\\boldsymbol{g} = (u1, u2...)$\n",
    "<p>\n",
    "$ y = f(\\boldsymbol{g}) $\n",
    "\n",
    "\n",
    "and the derivatives of a vector of functions wrapping a vector of single variable functions:\n",
    "<p>\n",
    "$ y = \\boldsymbol{f}(\\boldsymbol{g}(x))$ :\n",
    "\n",
    "and the derivatives of a vector of a vector of functions wrapping a vector of single variable functions:\n",
    "<p>\n",
    "$\\boldsymbol{y} = \\begin{bmatrix} \n",
    "f_{1}(\\boldsymbol{g}(x)) \\\\ \n",
    "f_{2}(\\boldsymbol{g}(x)) \\\\ \n",
    "f_{3}(\\boldsymbol{g}(x)) \\\\ \n",
    "... \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "and the partial derivatives of a vector of a vector of functions wrapping a vector of vector-valued functions (notice the bolded x):\n",
    "<p>\n",
    "$\\boldsymbol{y} = \\begin{bmatrix} \n",
    "f_{1}(\\boldsymbol{g}(\\boldsymbol{x})) \\\\ \n",
    "f_{2}(\\boldsymbol{g}(\\boldsymbol{x})) \\\\ \n",
    "f_{3}(\\boldsymbol{g}(\\boldsymbol{x})) \\\\ \n",
    "... \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "**What's the punchline?**\n",
    "\n",
    "With neural networks, sometimes it works out where the expressions are this complicated. BUT not to worry! Using the combination of chain rules and scalar derivative rules and vector addition rules (not discussed here but mentioned in [9]), **deep learning libraries do all the differentiation for us**. \n",
    "<p>\n",
    "\n",
    "If you follow the  breakdown of the mathematics in [9], we can decompose any derivative using something known as the _vector chain rule_, which reduces to a matrix multiplication where each matrix is mostly 0s and the diagonals are partial derivatives, which you can see below:\n",
    "\n",
    "<img src=\"vector_chain_rule.jpg\" width=\"600\" height=\"480\" />\n",
    "\n",
    "\n",
    "I think it's appropriate to insert the following gif of the mind being blown [11]:\n",
    "\n",
    "<img src=\"mind_blown.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. References\n",
    "\n",
    "<pre>\n",
    "  [1] Fast.ai (http://course.fast.ai/)  \n",
    "  [2] CS231N (http://cs231n.github.io/)  \n",
    "  [3] CS224D (http://cs224d.stanford.edu/syllabus.html)  \n",
    "  [4] Hands on Machine Learning (https://github.com/ageron/handson-ml)  \n",
    "  [5] Deep learning with Python Notebooks (https://github.com/fchollet/deep-learning-with-python-notebooks)  \n",
    "  [6] Deep learning by Goodfellow et. al (http://www.deeplearningbook.org/)  \n",
    "  [7] Neural networks online book (http://neuralnetworksanddeeplearning.com/)\n",
    "  [8] Vector Norms https://machinelearningmastery.com/vector-norms-machine-learning/\n",
    "  [9] The Matrix Calculus You Need For Deep Learning https://arxiv.org/pdf/1802.01528.pdf\n",
    "  [10] Practical Guide to Matrix Calculus for Deep Learning http://www.psi.toronto.edu/~andrew/papers/matrix_calculus_for_learning.pdf\n",
    "  [11] https://giphy.com/explore/mind-blown\n",
    "  [12] http://wiki.fast.ai/index.php/Gradient_Descent\n",
    "</pre>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
